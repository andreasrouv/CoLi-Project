{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents= ['Cristiano Ronaldo and Lionel Messi are both great player of football', 'People also admire Neymar and Ramos for their football skills',\n",
    "'USA and China both are powerful countries', 'China is building a strong economy', 'India is also emerging as one of most developing country by promoting football at global scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in documents:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [document.split() for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['also', 'one'])\n",
    "documents = [[word for word in text if word not in stop_words] for text in documents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = spacy.load('en_core_web_sm') #needs downloading beforehand\n",
    "lemmatized_docs = []\n",
    "for d in documents: \n",
    "    lemmas= [rules(word)[0].lemma_ for word in d]\n",
    "    lemmatized_docs.append(lemmas)\n",
    "    documents = lemmatized_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in documents:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_word_id(docs):\n",
    "    set_of_words = sorted(list(set([word for doc in docs for word in doc])))\n",
    "    \n",
    "\n",
    "    id_dictionary= {word: id for id,  word in enumerate(set_of_words)}\n",
    "    return id_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id =assign_word_id(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_of_words = [word for doc in documents for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ls_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict=defaultdict(int)\n",
    "for word in sorted(ls_of_words):\n",
    "    frequency_dict[word] +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frequency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_word_calculate(docs,z, num_of_topics):\n",
    "    topic_word = torch.zeros((num_of_topics, len(word2id)))\n",
    "    for d, doc in enumerate(docs):\n",
    "        for n, word in enumerate(doc):\n",
    "            word_id= word2id.get(word)\n",
    "            \n",
    "            \n",
    "            topic_id = z[d][n]\n",
    "            topic_word[topic_id][word_id] +=1\n",
    "    return topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapsed_Gibbs(docs, num_of_topics=4, passes =5, alpha= 0.1, beta=0.01):\n",
    "    max_len = max([len(d) for d in docs])\n",
    "        \n",
    "    z = torch.randint(0, num_of_topics, (len(docs), max_len)) #assigns word of each document to topic\n",
    "\n",
    "    for d, doc in enumerate(docs):\n",
    "        for i in range(len(doc), max_len):\n",
    "            z[d][i] = num_of_topics\n",
    "    \n",
    "\n",
    "    document_topic = torch.nn.functional.one_hot(z).sum(dim=1) #counts words assigned to each topic per document (Nd,k)\n",
    "    document_topic= document_topic[:,:num_of_topics]\n",
    "    words_per_topic =  torch.sum(document_topic, dim=0)#(Nk)\n",
    "    topic_word = topic_word_calculate(docs, z, num_of_topics) #counts which word is assigned per topic (Nk,w)\n",
    "    \n",
    "\n",
    "    for p in range(passes):\n",
    "        for d, doc in enumerate(docs):\n",
    "            for w, word in enumerate(doc):\n",
    "                word_id = word2id.get(word)\n",
    "                topic_id = z[d][w]\n",
    "                document_topic[d][topic_id] -=1\n",
    "                topic_word[topic_id][word_id] -=1\n",
    "                words_per_topic[topic_id] -=1\n",
    "                \n",
    "            \n",
    "                p = torch.zeros(num_of_topics)\n",
    "                for topic in range(num_of_topics):\n",
    "                    p[topic]= ((topic_word[topic][word_id] +beta) * (document_topic[d][topic]+ alpha)) /(words_per_topic[topic] + beta * len(word2id))\n",
    "                \n",
    "\n",
    "                #p /= p.sum() #Normalize the probability vector \n",
    "                \n",
    "                topic = torch.multinomial(p, 1, replacement=True).item() \n",
    "                #print(topic)\n",
    "                z[d][w] = topic \n",
    "                #print(p)\n",
    "                document_topic[d][topic] +=1\n",
    "                topic_word[topic][word_id] +=1\n",
    "                words_per_topic[topic] +=1\n",
    "        #print(document_topic)\n",
    "            \n",
    "    return document_topic, topic_word, words_per_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic, topic_word, words_per_topic= collapsed_Gibbs(documents, 2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_topic, topic_word)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
